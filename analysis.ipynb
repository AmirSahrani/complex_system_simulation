{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-criticality in the Cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setup notebook environment -q flag suppresses output, if you want to see it, remove the -q flag'''\n",
    "# %pip install -r requirements.txt -q\n",
    "from utils.plotting_utils import *\n",
    "from utils.data_utils import *\n",
    "from utils.utils import *\n",
    "from branching import BranchingNeurons\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import powerlaw\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "import powerlaw\n",
    "from sandpile import BTW\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "file = 'data/branching_data_final.csv'\n",
    "if not os.path.exists(file) or TESTING:\n",
    "    if TESTING:\n",
    "        os.remove(file)\n",
    "    for branching_ratio in tqdm(np.logspace(np.log10(0.5), np.log10(5), 20)):\n",
    "        kwargs = {\n",
    "            'N': 2500,\n",
    "            'max_neighbors': 28,\n",
    "            'branching_ratio': branching_ratio,\n",
    "            'visual': False,\n",
    "        }\n",
    "        data = simulate(BranchingNeurons, n_runs=10, duration=10000, **kwargs)\n",
    "        write_data(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file, header=0, index_col=0)\n",
    "data['mean_density'] = data['density'].apply(float)\n",
    "data['evalanche_duration'] = data['evalanche_duration'].apply(str_to_list)\n",
    "data['evalanche_size'] = data['evalanche_size'].apply(str_to_list)\n",
    "\n",
    "data['density_duration'] = data.apply(lambda x: get_density(x['evalanche_duration'])[0], axis=1)\n",
    "data['density_size'] = data.apply(lambda x: get_density(x['evalanche_size'])[0], axis=1)\n",
    "data['values_duration'] = data.apply(lambda x: get_density(x['evalanche_duration'])[1], axis=1)\n",
    "data['values_size'] = data.apply(lambda x: get_density(x['evalanche_size'])[1], axis=1)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_branching = data.groupby('branching_ratio').mean(numeric_only=True)\n",
    "critical_point = closest_index_to_value(grouped_branching.index, 1)\n",
    "critical_data = grouped_branching.loc[grouped_branching.index == grouped_branching.index[critical_point]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'N': 500,\n",
    "    'max_neighbors': 28,\n",
    "    'branching_ratio': None,\n",
    "    'visual': False,\n",
    "}\n",
    "plot_activity_per_time_step(10000, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(grouped_branching.index, grouped_branching['emperical_branching_ratio'], label='Branching ratio')\n",
    "plt.plot(np.linspace(np.min(grouped_branching.index), np.max(grouped_branching.index), 100), np.linspace(0, np.max(grouped_branching.index), 100), label='1:1 line', color='black', linestyle='--' , alpha=0.5)\n",
    "plt.xlabel('Branching ratio')\n",
    "plt.ylabel('Emperical branching ratio')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grouped_branching.index, grouped_branching['mean_density'])\n",
    "plt.scatter(grouped_branching.index, grouped_branching['mean_density'])\n",
    "\n",
    "plt.scatter(grouped_branching.index.values[critical_point], grouped_branching['mean_density'].values[critical_point], c='r')\n",
    "plt.text(grouped_branching.index.values[critical_point] + 0.2, grouped_branching['mean_density'].values[critical_point], 'Critical Point')\n",
    "plt.xlabel('Branching Ratio')\n",
    "plt.xlim(0.5, 3)\n",
    "plt.ylabel('Mean Density')\n",
    "plt.title('Mean Density vs Branching Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loglog_plotting(type: str, data: pd.DataFrame, grouped_branching: pd.DataFrame):\n",
    "    fig, ax =plt.subplots(3,3, figsize=(15,15))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(9):\n",
    "        offset = 4\n",
    "        all_critical_points =  data.loc[data['branching_ratio'] == grouped_branching.index[offset+i]]\n",
    "        all_data = np.concatenate(all_critical_points[type].values)\n",
    "        all_data = all_data[all_data > 0]\n",
    "\n",
    "        fit = powerlaw.Fit(all_data, verbose=False)\n",
    "        x_values = np.linspace(0, max(all_data), 100)\n",
    "        fitted_line =  fit.xmin*x_values ** -fit.alpha\n",
    "\n",
    "\n",
    "\n",
    "        powerlaw.plot_pdf(all_data, ax=ax[i], color='red', label='Empirical data' , linestyle='None', marker='o', markersize=3, alpha=0.5)\n",
    "        ax[i].plot(fitted_line, color='black', linestyle='--', label='Power law fit')\n",
    "\n",
    "        ax[i].set_title(f'Branching ratio: {grouped_branching.index[offset+i]:.2f}')\n",
    "        ax[i].text(0.6, 0.9, f'Alpha: {fit.alpha:.2f}\\n ', transform=ax[i].transAxes)\n",
    "        ax[i].set_xlabel(type.split('_')[0].capitalize() + ' ' + type.split('_')[1])\n",
    "        ax[i].set_ylabel('Frequency')\n",
    "        ax[i].set_xscale('log')\n",
    "        ax[i].set_yscale('log')\n",
    "        ax[i].set_xlim([1, 1e3])\n",
    "        ax[i].set_ylim([1e-5, 1e0])\n",
    "\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loglog_plotting('evalanche_duration', data, grouped_branching)\n",
    "loglog_plotting('evalanche_size', data, grouped_branching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTW-like model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df: index, spikes_total, spikes_input, spikes_neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings1 = [\n",
    "    {\"name\": \"round_spiral\", \"params\": {\"height\": 4, \"refractory_period\": 5, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": False}},\n",
    "    {\"name\": \"pulse_wave\", \"params\": {\"height\": 5, \"refractory_period\": 3, \"probability_of_spontaneous_activity\": 0.03, \"max_distance\": 3, \"random_connection\": False}},\n",
    "    {\"name\": \"synchronous\", \"params\": {\"height\": 3, \"refractory_period\": 5, \"probability_of_spontaneous_activity\": 0.015, \"max_distance\": 2.5, \"random_connection\": True}},\n",
    "    {\"name\": \"oscillatory\", \"params\": {\"height\": 2, \"refractory_period\": 4, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": False}},\n",
    "    {\"name\": \"repeating\", \"params\": {\"height\": 2, \"refractory_period\": 4, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": True}},\n",
    "    {\"name\": \"random\", \"params\": {\"height\": 5, \"refractory_period\": 5, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": False}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection to csv in df: spikes_input, spikes_neighbours, spikes_total per time step\n",
    "for setting in settings1:\n",
    "    btw = BTW(grid_size=[50, 50], **setting['params'])\n",
    "    btw.run(10000)\n",
    "    path = f\"data/spikes_btw_{setting['name']}.csv\"\n",
    "    btw.write_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what's the pattern like for different patterns\n",
    "paths = [f\"data/spikes_btw_{setting['name']}.csv\" for setting in settings1]\n",
    "for path in paths:\n",
    "    df = load_data_csv(path)\n",
    "    sigma = branching_prameter(df)\n",
    "    print(path, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more parameters settings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df: avalanche_size, avalanche_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to store raster data, which is not used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in settings2:\n",
    "    btw = BTW(grid_size=[50, 50], **setting['params'])\n",
    "    btw.init_grid(\"random\", 4)\n",
    "    btw.run(10000)\n",
    "    path = f\"data/spikes_btw_avalanche/spikes_btw_{setting['name']}.csv\"\n",
    "    btw.collect_raster_data(10000, path) # This function: collect raster data was once used to collect raster data in csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to store raster data for different patterns, which is also used plot spike activity figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in settings1:\n",
    "    btw = BTW(grid_size=[20, 20], ** setting['params'])\n",
    "    btw.init_grid(\"random\", 4)\n",
    "    btw.run(1500)\n",
    "    path = f\"data/spikes_btw_avalanche_grid/spikes_btw_{setting['name']}.csv\"\n",
    "    df = btw.collect_raster_data(5000)\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to store avalanche data, which is also used to plot power law distributions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings3 = [{\"name\": f\"ref{ref}thresh{thresh}p{p}\", \n",
    "            \"params\": {\"height\": thresh, \n",
    "                        \"refractory_period\": ref, \n",
    "                        \"probability_of_spontaneous_activity\": p, \n",
    "                        \"max_distance\": 3, \n",
    "                        \"random_connection\": False}}\n",
    "            for ref in range(2, 8) for thresh in range(2, 8) for p in [0.015, 0.02, 0.025]]\n",
    "paths_avalanche = []\n",
    "settings4 = []\n",
    "for setting in settings3:\n",
    "    btw = BTW(grid_size=[20, 20], **setting['params'])\n",
    "    btw.init_grid(\"random\", 4)\n",
    "    btw.run(10000)\n",
    "    df_raster = btw.collect_raster_data(3000)\n",
    "    df = raster_to_basic(df_raster)\n",
    "    sigma = branching_prameter(df)\n",
    "    print(sigma)\n",
    "    if(abs(sigma-1) < 0.05):\n",
    "        settings4.append(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(abs(branching_prameter(df)-1) < 0.05):\n",
    "        df_transmission = raster_to_transmission(df_raster)\n",
    "        avalanche = transmission_to_avalanche(df_transmission)\n",
    "        avalanche_df = avalanche_to_statistics(avalanche)\n",
    "        avalanche_df.to_csv(path, index=True)    \n",
    "        paths_avalanche.append(path)\n",
    "        print(\"Data written to:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Spike Density vs. Branching Ratio m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg_spike_density vs. m\n",
    "settings2 = [{\"name\": f\"ref{ref}thresh{thresh}p{p}r{False}\", \n",
    "            \"params\": {\"height\": thresh, \n",
    "                        \"refractory_period\": ref, \n",
    "                        \"probability_of_spontaneous_activity\": p, \n",
    "                        \"max_distance\": 3, \n",
    "                        \"random_connection\": False}}\n",
    "            for ref in range(1, 8) for thresh in range(1, 7) for p in [0.015,0.02,0.025]]\n",
    "# The write data is neglected in this ipynb for clarification. It's similar to what we did to the settings1.\n",
    "paths = [f\"data/spikes_btw_ref_thresh/spikes_btw_{setting['name']}.csv\" for setting in settings2]\n",
    "size = 50\n",
    "spike_density_plot(paths, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg_spike_density vs. m considering refractory period \n",
    "settings2 = [{\"name\": f\"ref{ref}thresh{thresh}p{p}r{False}\", \n",
    "            \"params\": {\"height\": thresh, \n",
    "                        \"refractory_period\": ref, \n",
    "                        \"probability_of_spontaneous_activity\": p, \n",
    "                        \"max_distance\": 3, \n",
    "                        \"random_connection\": False}}\n",
    "            for ref in range(1, 8) for thresh in range(1, 7) for p in [0.015,0.02,0.025]]\n",
    "# The write data is neglected in this ipynb for clarification. It's similar to what we did to the settings1.\n",
    "paths = [f\"data/spikes_btw_ref_thresh/spikes_btw_{setting['name']}.csv\" for setting in settings2]\n",
    "refs = [setting['params']['refractory_period'] for setting in settings2]\n",
    "size = 50\n",
    "ref_spike_density_plot(paths, size, refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avalanche size/duration distribution (distinguishing origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avalanches = []\n",
    "for i in range(20):\n",
    "    # This is a parameter setting we selcted which sigma is near 1.\n",
    "    btw = BTW(grid_size=[50, 50], refractory_period=3,height=2,probability_of_spontaneous_activity=0.02,max_distance=3,random_connection=False)\n",
    "    raster_df = btw.collect_raster_data(1000)\n",
    "    df = raster_to_basic(raster_df)\n",
    "    df_transmission = raster_to_transmission(raster_df)\n",
    "    avalanche = transmission_to_avalanche(df_transmission)\n",
    "    avalanche_df = avalanche_to_statistics(avalanche)\n",
    "    avalanches.append(avalanche)\n",
    "all_avalanches = [item for sublist in avalanches for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avalanche_df = avalanche_to_statistics(all_avalanches)\n",
    "avalanche_df.to_csv(\"data/avalanche_powerlaw_df.csv\", index=True)\n",
    "avalanches_by_length = defaultdict(list)\n",
    "for av in all_avalanches:\n",
    "    avalanches_by_length[len(av)].append(av)\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data_csv(\"data/avalanche_powerlaw_df.csv\", index = True)\n",
    "avalanche_sizes = df['avalanche_size'].tolist()\n",
    "avalanche_sizes = [sum(av) for av in all_avalanches if len(av) > 0]\n",
    "fit = powerlaw.Fit(avalanche_sizes)\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "plt.style.use('tableau-colorblind10') \n",
    "\n",
    "fit.plot_pdf(ax=ax,linestyle='None', marker='o', markersize=4, label='Original Distribution')\n",
    "fit.power_law.plot_pdf(linestyle='--', ax=ax, label='Powerlaw Fitting Distribution')\n",
    "#original_line = plt.Line2D([], [], color='b', label='Original Distribution')\n",
    "#powerlaw_line = plt.Line2D([], [], color='r', linestyle='--', label='Powerlaw Fitting Distribution')\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Avalanche Size\", fontsize=16)\n",
    "plt.ylabel(\"Probability Density\", fontsize=16)\n",
    "plt.title(\"Avalanche Size Distribution\", fontsize=17)\n",
    "plt.show()\n",
    "print('Tau (τ) - Power-law exponent:', fit.power_law.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f\"data/spikes_btw_avalanche_grid/spikes_btw_synchronous.csv\"]# Change the path to get diffenrent activity plots\n",
    "spike_activity_plot(paths, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Density vs timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings3 = [{\"name\": f\"ref{ref}thresh{t}p{p}r{r}\", \n",
    "            \"params\": {\"height\": t, \n",
    "                        \"refractory_period\": ref, \n",
    "                        \"probability_of_spontaneous_activity\": p, \n",
    "                        \"max_distance\": 3, \n",
    "                        \"random_connection\": r}}\n",
    "            for ref in range(1, 8) for t in range(1, 8) for p in [0.015, 0.02, 0.025] for r in [False, True]]\n",
    "paths = [f\"data/spikes_btw_ref_thresh/spikes_btw_{setting['name']}.csv\" for setting in settings3]\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    r = branching_prameter(df)\n",
    "    if(abs(r - 1) < 0.05):\n",
    "        print(\"paths: \", path)\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/spikes_btw_ref_thresh/spikes_btw_ref2thresh3p0.02rFalse.csv\",\n",
    "         \"data/spikes_btw_ref_thresh/spikes_btw_ref4thresh4p0.015rFalse.csv\",\n",
    "         \"data/spikes_btw_ref_thresh/spikes_btw_ref6thresh6p0.015rTrue.csv\"]\n",
    "size = 50\n",
    "grid_activity_timestep(paths, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale-free Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avalanches = []\n",
    "for i in range(10):\n",
    "    btw = BTW(grid_size=[50, 50], refractory_period=3,height=2,probability_of_spontaneous_activity=0.02,max_distance=3,random_connection=False)\n",
    "    raster_df = btw.collect_raster_data(1000)\n",
    "    df = raster_to_basic(raster_df)\n",
    "    df_transmission = raster_to_transmission(raster_df)\n",
    "    avalanche = transmission_to_avalanche(df_transmission)\n",
    "    avalanches.append(avalanche)\n",
    "all_avalanches = [item for sublist in avalanches for item in sublist]\n",
    "print(all_avalanches)\n",
    "avalanches_by_length = defaultdict(list)\n",
    "for av in all_avalanches:\n",
    "    avalanches_by_length[len(av)].append(av)\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the all_avalnches to csv\n",
    "avalanche_df = avalanche_to_statistics(all_avalanches)\n",
    "avalanche_df.to_csv(\"data/avalanche_statistics_scalefree.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avalanche size vs. avalanche duration for scale-fee property\n",
    "df = load_data_csv(\"data/avalanche_statistics_scalefree.csv\")\n",
    "avalanches_by_length = defaultdict(list)\n",
    "for av in all_avalanches:\n",
    "    avalanches_by_length[len(av)].append(av)\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}\n",
    "plt.figure(figsize=(6, 5)) \n",
    "plt.title(\"Scale-free Property\", fontsize=14)\n",
    "plt.xlabel(\"Avalanche Durations\", fontsize=14)\n",
    "plt.ylabel(\"Avalanche Sizes\", fontsize=14)\n",
    "plt.grid(True)\n",
    "for length, mean_activity in mean_activities.items():\n",
    "    if length < 17 and length > 3:\n",
    "        plt.plot(range(length), mean_activity, label=f'Duration {length}', color='blue', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerlaw scatter for sizes\n",
    "avalanche_sizes = [sum(av) for av in all_avalanches]\n",
    "size_counts = Counter(avalanche_sizes)\n",
    "sizes, counts = zip(*size_counts.items())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Avalanche Size Distribution\")\n",
    "plt.xlabel(\"Avalanche size (log scale)\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "log_sizes = np.log(sizes)\n",
    "log_counts = np.log(counts)\n",
    "\n",
    "plt.scatter(log_sizes, log_counts, color='blue', marker='o')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerlaw fitting for sizes\n",
    "avalanche_sizes = [sum(av) for av in all_avalanches if len(av) > 0]  \n",
    "\n",
    "fit = powerlaw.Fit(avalanche_sizes)\n",
    "fig = fit.plot_pdf(color='b', linewidth=2, label=\"Original Distribution\")\n",
    "fit.power_law.plot_pdf(color='r', linestyle='--', ax=fig, label=\"Powerlaw Fitting Distribution\")\n",
    "t = fit.power_law.alpha\n",
    "plt.xlabel(\"Avalanche Size\", fontsize=14)\n",
    "plt.ylabel(\"Probability Density\", fontsize=14)\n",
    "plt.title(\"Avalanche Dize Distribution\", fontsize=16)\n",
    "plt.legend(fontsize = 13)\n",
    "\n",
    "print('Tau (τ) - Power-law exponent:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}\n",
    "durations = []\n",
    "sizes = []\n",
    "for duration, activities in mean_activities.items():\n",
    "    if duration < 17 and duration > 5:\n",
    "        durations.append(duration)\n",
    "        sizes.append(np.mean(activities))\n",
    "log_durations = np.log(durations).reshape(-1, 1)\n",
    "log_sizes = np.log(sizes)\n",
    "model = LinearRegression()\n",
    "model.fit(log_durations, log_sizes)\n",
    "predicted = model.predict(log_durations)\n",
    "gamma = model.coef_[0]\n",
    "r2 = r2_score(log_sizes, predicted)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(log_durations, log_sizes, color='black', label='Data points')\n",
    "plt.plot(log_durations, predicted, color='red', label=f'Linear fit: γ = {gamma:.2f}, R² = {r2:.2f}')\n",
    "plt.legend()\n",
    "plt.xlabel('Log(duration)')\n",
    "plt.ylabel('Log(size)')\n",
    "plt.title('Log-Log Plot of Avalanche Duration vs Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the scalefree with zooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5)) \n",
    "plt.title(\"Scale-free Property\", fontsize=14)\n",
    "plt.xlabel(\"Avalanche Durations\", fontsize=14)\n",
    "plt.ylabel(\"Avalanche Sizes\", fontsize=14)\n",
    "plt.grid(True)\n",
    "for length, mean_activity in mean_activities.items():\n",
    "    if length < 17 and length > 5 :\n",
    "        scaled_time = [i / (length - 1) for i in range(length)]\n",
    "        scaled_activity = [activity / ((length) ** (0.89 - 1)) for activity in mean_activity]\n",
    "        plt.plot(scaled_time, scaled_activity, label=f'Duration {length}', color='blue', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 3, 3]\n",
    "b = [0, 3, 2]\n",
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "args = {\"one\": 4, \"two\": 5, \"three\": 0.02}\n",
    "args_df = pd.DataFrame(args, index=[0])\n",
    "results_df = pd.DataFrame({\"spikes_total\": np.array([0, 3, 3]), \n",
    "                        \"spikes_neighbours\": np.array([0, 1, 1]), \n",
    "                        \"spikes_input\": np.array([0, 3, 3]) - np.array([0, 1, 1])})\n",
    "combined_df = pd.concat([args_df, results_df], axis=1)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_data_csv(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic for presentation:\n",
    "### Motivation\n",
    "### Data we use: self-generated?\n",
    "### Hypothesis\n",
    "### What we did:\n",
    "#### CA Model: \n",
    "##### How is this Model like？\n",
    "1. Rules: \n",
    "   1. We have a 2d grid. \n",
    "   2. Interact rule: The neighbour neurons around spikes are going to be added 1 to their grid number. If in ths time step, neurons who get enough gird number, which means reaches the threshold, becomes a spike_neighbour in the next time step. And if they donot reach the threshold, their grid numbers will be cleared to zero.\n",
    "   3. Refractory period: After 1 timestep, a spike will get into the refractory period, which means within the next refractoy period timesteps, the spike will get activated whatever happens.\n",
    "   4. Spontaneous spike rule: We have this spontaneous_input_probability p to make random unactivated and also not in refractory period neurons to activate and become a spike_input.\n",
    "   5. So, we have a big timestep for loop, we first make all spikes into refractory period and also make spikes in refractory period in time -1 refractory period. This will take effect at the next timestep. Then we check neighbours to create spikes_neighbours and then we add_grains to make spikes_input.\n",
    "2. Parameters(max_height,refractory_period,spontaneous_input_probability,random_connection，max_distance)\n",
    "##### What we get: \n",
    "1. By playing with the parameters tuning, we find patterns(vedios and raster figure). \n",
    "2. While running on different settings of parameters, we find the phase transition around branching ratio =1.\n",
    "3. While changing only one parameter (let's say a) each time, we find the interesting relationship beween spike_density and a. In principle, what we are doing is changing branching ratio. However, the relationship is different for different a. You see it looks like first-order phase transition, but we think actually it's not. It may be just because of our activation rule is adding 1 to your neighbours at one time, but not 0.3 or 0.5. So the data we get is discrete.\n",
    "4. We tried to delve into the powerlaw distribution of avalanche size and distribution and the relationship between these 2 /tao, but the results we get is for avalanche size, it clearly follows powerlaw distribution, but for avalanche duration distribution, it seems not. We think it's due to in the CA model we implemented, the avalanches tend to merge into one big avalanche. Even though we have implemented a complicated algorithm to track avalanche origins, we still cannot prevent avalanches merging into one big avalanche. So the avalanche data we get often ends with one very big avalanche size and duration number. \n",
    "So we want to implement this branching model with random network to make up for the shortcomings of existing models in studying avalanche size and duration distribution.\n",
    "#### Branching model\n",
    "How is the model like: Rule, Parameters\n",
    "What we get:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputationalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
