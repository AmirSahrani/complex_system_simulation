{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-criticality in the Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setup notebook environment -q flag suppresses output, if you want to see it, remove the -q flag'''\n",
    "# %pip install -r requirements.txt -q\n",
    "from utils.plotting_utils import *\n",
    "from utils.data_utils import *\n",
    "from utils.utils import *\n",
    "from branching import BranchingNeurons\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import powerlaw\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from sandpile import BTW\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TESTING = True \n",
    "file = 'data/branching_data_densities.csv'\n",
    "if not os.path.exists(file) or TESTING:\n",
    "    if TESTING:\n",
    "        os.remove(file)\n",
    "    for branching_ratio in tqdm(np.logspace(np.log10(0.5), np.log10(5), 20)):\n",
    "        for i in [0,1,3,5]:\n",
    "            kwargs = {\n",
    "                'N': 100,\n",
    "                'max_neighbors': 28,\n",
    "                'cooldown': i,\n",
    "                'branching_ratio': branching_ratio,\n",
    "                'visual': False,\n",
    "            }\n",
    "            data = simulate(BranchingNeurons, n_runs=10, duration=10000, **kwargs)\n",
    "            write_data(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "file = 'data/branching_data_final.csv'\n",
    "if not os.path.exists(file) or TESTING:\n",
    "    if TESTING:\n",
    "        pass\n",
    "    for branching_ratio in tqdm(np.logspace(np.log10(0.5), np.log10(5), 20)):\n",
    "        kwargs = {\n",
    "            'N': 2500,\n",
    "            'max_neighbors': 28,\n",
    "            'branching_ratio': branching_ratio,\n",
    "            'visual': False,\n",
    "        }\n",
    "        data = simulate(BranchingNeurons, n_runs=10, duration=10000, **kwargs)\n",
    "        write_data(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/branching_data_final.csv', header=0, index_col=0)\n",
    "data['mean_density'] = data['density'].apply(float)\n",
    "data['evalanche_duration'] = data['evalanche_duration'].apply(str_to_list)\n",
    "data['evalanche_size'] = data['evalanche_size'].apply(str_to_list)\n",
    "\n",
    "data['density_duration'] = data.apply(lambda x: get_density(x['evalanche_duration'])[0], axis=1)\n",
    "data['density_size'] = data.apply(lambda x: get_density(x['evalanche_size'])[0], axis=1)\n",
    "data['values_duration'] = data.apply(lambda x: get_density(x['evalanche_duration'])[1], axis=1)\n",
    "data['values_size'] = data.apply(lambda x: get_density(x['evalanche_size'])[1], axis=1)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_branching = data.groupby('branching_ratio').mean(numeric_only=True)\n",
    "critical_point = closest_index_to_value(grouped_branching.index, 1)\n",
    "critical_data = grouped_branching.loc[grouped_branching.index == grouped_branching.index[critical_point]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'N': 500,\n",
    "    'max_neighbors': 28,\n",
    "    'branching_ratio': None,\n",
    "    'visual': False,\n",
    "}\n",
    "plot_activity_per_time_step(10000, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(grouped_branching.index, grouped_branching['emperical_branching_ratio'], label='Branching ratio')\n",
    "plt.plot(np.linspace(np.min(grouped_branching.index), np.max(grouped_branching.index), 100), np.linspace(0, np.max(grouped_branching.index), 100), label='1:1 line', color='black', linestyle='--' , alpha=0.5)\n",
    "plt.xlabel('Branching ratio')\n",
    "plt.ylabel('Emperical branching ratio')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cooldowns = pd.read_csv('data/branching_data_densities.csv', header=0, index_col=0)\n",
    "grouped_cooldowns = data_cooldowns.groupby(['cooldown', 'branching_ratio']).agg({'density': ['mean', 'std']})\n",
    "\n",
    "for i in [0,1,3,5]:\n",
    "    plt.plot(grouped_cooldowns.loc[i].index, grouped_cooldowns.loc[i]['density']['mean'], label=f'Cooldown {i}')\n",
    "    plt.fill_between(grouped_cooldowns.loc[i].index, grouped_cooldowns.loc[i]['density']['mean'] - grouped_cooldowns.loc[i]['density']['std'], grouped_cooldowns.loc[i]['density']['mean'] + grouped_cooldowns.loc[i]['density']['std'], alpha=0.2)\n",
    "plt.xlabel('Branching ratio')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0.5, 5)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grouped_branching.index, grouped_branching['mean_density'])\n",
    "plt.scatter(grouped_branching.index, grouped_branching['mean_density'])\n",
    "\n",
    "plt.scatter(grouped_branching.index.values[critical_point], grouped_branching['mean_density'].values[critical_point], c='r')\n",
    "plt.text(grouped_branching.index.values[critical_point] + 0.2, grouped_branching['mean_density'].values[critical_point], 'Critical Point')\n",
    "plt.xlabel('Branching Ratio')\n",
    "plt.xlim(0.5, 3)\n",
    "plt.ylabel('Mean Density')\n",
    "plt.title('Mean Density vs Branching Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loglog_plotting(type: str, data: pd.DataFrame, grouped_branching: pd.DataFrame):\n",
    "    fig, ax =plt.subplots(3,2, figsize=(15,15))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(6):\n",
    "        offset = 4\n",
    "\n",
    "        all_critical_points =  data.loc[data['branching_ratio'] == grouped_branching.index[offset+i]]\n",
    "        all_data = np.concatenate(all_critical_points[type].values)\n",
    "        all_data = all_data[all_data > 0]\n",
    "\n",
    "        fit = powerlaw.Fit(all_data, verbose=False)\n",
    "        lognormal = powerlaw.Lognormal(verbose=False)\n",
    "        lognormal.fit(all_data)\n",
    "        mu, sigma = lognormal.mu, lognormal.sigma\n",
    "\n",
    "        log_llkhood, p_value = fit.distribution_compare('power_law', 'lognormal', normalized_ratio=True)\n",
    "\n",
    "        x_values = np.linspace(min(all_data), max(all_data), len(all_data))\n",
    "        fitted_line = (x_values ** -fit.alpha)\n",
    "        log_fitted = lognormal.pdf(x_values)\n",
    "        \n",
    "\n",
    "        powerlaw.plot_pdf(all_data, ax=ax[i], color='red', label='Empirical data' , linestyle='None', marker='o', markersize=3, alpha=0.5)\n",
    "        ax[i].plot(x_values, fitted_line, color='black', linestyle='--', label='Power law fit')\n",
    "        ax[i].plot(x_values, log_fitted, color='blue', linestyle='--', label='Log Normal fit')\n",
    "\n",
    "        ax[i].set_title(f'Branching ratio: {grouped_branching.index[offset+i]:.2f}')\n",
    "        ax[i].text(0.1, 0.1, \n",
    "           f'$\\\\alpha$: {fit.alpha:.2f}\\n$\\\\mu$: {mu:.2f}\\n$\\\\sigma$: {sigma:.2f}\\n$p$: {p_value:.3f}\\nLog Likelihood: {log_llkhood:.3f}', \n",
    "           transform=ax[i].transAxes)\n",
    "\n",
    "        ax[i].set_xlabel(type.split('_')[0].capitalize() + ' ' + type.split('_')[1])\n",
    "        ax[i].set_ylabel('Frequency')\n",
    "        \n",
    "        if i == 1:\n",
    "            ax[i].legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loglog_plotting_size_duration(type: List[str], data: pd.DataFrame, grouped_branching: pd.DataFrame):\n",
    "    fig, ax =plt.subplots(3,2, figsize=(15,15))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(6):\n",
    "        offset = 4\n",
    "\n",
    "        all_data=  data.loc[data['branching_ratio'] == grouped_branching.index[offset+i]]\n",
    "\n",
    "        all_size = np.concatenate(all_data[type[0]].values)\n",
    "        all_size = all_size[all_size > 0]\n",
    "\n",
    "        all_duration = np.concatenate(all_data[type[1]].values)\n",
    "        all_duration = all_duration[all_duration > 0]\n",
    "\n",
    "\n",
    "\n",
    "        ax[i].set_title(f'Branching ratio: {grouped_branching.index[offset+i]:.2f}')\n",
    "        ax[i].loglog(all_size, all_duration, color='red', linestyle='None', marker='o', markersize=3, alpha=0.5)\n",
    "        ax[i].set_xlabel(type[0].split('_')[0].capitalize() + ' ' + type[0].split('_')[1])\n",
    "        ax[i].set_xlabel(type[1].split('_')[0].capitalize() + ' ' + type[1].split('_')[1])\n",
    "\n",
    "        if i == 1:\n",
    "            ax[i].legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglog_plotting(\"evalanche_duration\", data, grouped_branching)\n",
    "loglog_plotting(\"evalanche_size\", data, grouped_branching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglog_plotting_size_duration(['evalanche_size', 'evalanche_duration'], data, grouped_branching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTW-like model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df: index, spikes_total, spikes_input, spikes_neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings1 = [\n",
    "    {\"name\": \"round_spiral\", \"params\": {\"height\": 4, \"refractory_period\": 5, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": False}},\n",
    "    {\"name\": \"pulse_wave\", \"params\": {\"height\": 5, \"refractory_period\": 4, \"probability_of_spontaneous_activity\": 0.03, \"max_distance\": 3, \"random_connection\": False}},\n",
    "    {\"name\": \"synchronous\", \"params\": {\"height\": 3, \"refractory_period\": 5, \"probability_of_spontaneous_activity\": 0.015, \"max_distance\": 2.5, \"random_connection\": True}},\n",
    "    {\"name\": \"oscillatory\", \"params\": {\"height\": 2, \"refractory_period\": 4, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": False}},\n",
    "    {\"name\": \"repeating\", \"params\": {\"height\": 2, \"refractory_period\": 4, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": True}},\n",
    "    {\"name\": \"random\", \"params\": {\"height\": 5, \"refractory_period\": 5, \"probability_of_spontaneous_activity\": 0.02, \"max_distance\": 3, \"random_connection\": False}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection to csv in df: spikes_input, spikes_neighbours, spikes_total per time step\n",
    "for setting in settings1:\n",
    "    btw = BTW(grid_size=[50, 50], **setting['params'])\n",
    "    btw.run(10000)\n",
    "    path = f\"data/spikes_btw_{setting['name']}.csv\"\n",
    "    btw.write_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg_spike_density vs. m\n",
    "paths = [f\"data/spikes_btw_{setting['name']}.csv\" for setting in settings1]\n",
    "size = 50\n",
    "spike_density_plot(paths, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df: avalanche_size, avalanche_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to store raster data, which is not used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in settings2:\n",
    "    btw = BTW(grid_size=[50, 50], **setting['params'])\n",
    "    btw.init_grid(\"random\", 4)\n",
    "    btw.run(10000)\n",
    "    path = f\"data/spikes_btw_avalanche/spikes_btw_{setting['name']}.csv\"\n",
    "    btw.collect_raster_data(10000, path) # This function: collect raster data was once used to collect raster data in csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to store raster data for different patterns, which is also used plot spike activity figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in settings1:\n",
    "    btw = BTW(grid_size=[20, 20], ** setting['params'])\n",
    "    btw.init_grid(\"random\", 4)\n",
    "    btw.run(1500)\n",
    "    path = f\"data/spikes_btw_avalanche_grid/spikes_btw_{setting['name']}.csv\"\n",
    "    btw.collect_raster_data(5000, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to store avalanche data, which is also used to plot power law distributions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings3 = [{\"name\": f\"ref{ref}thresh{thresh}p{p}\", \n",
    "            \"params\": {\"height\": thresh, \n",
    "                        \"refractory_period\": ref, \n",
    "                        \"probability_of_spontaneous_activity\": p, \n",
    "                        \"max_distance\": 3, \n",
    "                        \"random_connection\": False}}\n",
    "            for ref in range(2, 8) for thresh in range(2, 8) for p in [0.015, 0.02, 0.025]]\n",
    "paths_avalanche = []\n",
    "settings4 = []\n",
    "for setting in settings3:\n",
    "    btw = BTW(grid_size=[20, 20], **setting['params'])\n",
    "    btw.init_grid(\"random\", 4)\n",
    "    btw.run(10000)\n",
    "    df_raster = btw.collect_raster_data(3000)\n",
    "    df = raster_to_basic(df_raster)\n",
    "    sigma = branching_prameter(df)\n",
    "    print(sigma)\n",
    "    if(abs(sigma-1) < 0.05):\n",
    "        settings4.append(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(abs(branching_prameter(df)-1) < 0.05):\n",
    "        df_transmission = raster_to_transmission(df_raster)\n",
    "        avalanche = transmission_to_avalanche(df_transmission)\n",
    "        avalanche_df = avalanche_to_statistics(avalanche)\n",
    "        avalanche_df.to_csv(path, index=True)    \n",
    "        paths_avalanche.append(path)\n",
    "        print(\"Data written to:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avalanche size/duration distribution (distinguishing origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btw = BTW(grid_size=[50, 50], refractory_period=3,height=2,probability_of_spontaneous_activity=0.02,max_distance=3,random_connection=False)\n",
    "raster_df = btw.collect_raster_data(1000)\n",
    "df = raster_to_basic(raster_df)\n",
    "print(branching_prameter(df))\n",
    "df_transmission = raster_to_transmission(df_raster)\n",
    "avalanche = transmission_to_avalanche(df_transmission)\n",
    "avalanche_df = avalanche_to_statistics(avalanche)\n",
    "print(avalanche_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_size = Counter(avalanche_df['size'])\n",
    "sizes, nums = [size for size, num in count_size.items()], [num for size, num in count_size.items()]\n",
    "plt.figure()\n",
    "plt.scatter(sizes, nums)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Avalanche size distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_duration = Counter(avalanche_df['duration'])\n",
    "durations, nums = [size for size, num in count_duration.items()], [num for size, num in count_duration.items()]\n",
    "plt.figure()\n",
    "plt.scatter(durations, nums)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Avalanche duration distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f\"data/spikes_btw_avalanche_grid/spikes_btw_random.csv\"]\n",
    "spike_activity_plot(paths, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Density vs timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings3 = [{\"name\": f\"ref{ref}thresh{t}p{p}r{r}\", \n",
    "            \"params\": {\"height\": t, \n",
    "                        \"refractory_period\": ref, \n",
    "                        \"probability_of_spontaneous_activity\": p, \n",
    "                        \"max_distance\": 3, \n",
    "                        \"random_connection\": r}}\n",
    "            for ref in range(1, 8) for t in range(1, 8) for p in [0.015, 0.02, 0.025] for r in [False, True]]\n",
    "paths = [f\"data/spikes_btw_ref_thresh/spikes_btw_{setting['name']}.csv\" for setting in settings3]\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    r = branching_prameter(df)\n",
    "    if(abs(r - 1) < 0.05):\n",
    "        print(\"paths: \", path)\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/spikes_btw_ref_thresh/spikes_btw_ref2thresh3p0.02rFalse.csv\",\n",
    "         \"data/spikes_btw_ref_thresh/spikes_btw_ref4thresh4p0.015rFalse.csv\",\n",
    "         \"data/spikes_btw_ref_thresh/spikes_btw_ref6thresh6p0.015rTrue.csv\"]\n",
    "size = 50\n",
    "grid_activity_timestep(paths, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale-free Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avalanches = []\n",
    "for i in range(10):\n",
    "    btw = BTW(grid_size=[50, 50], refractory_period=3,height=2,probability_of_spontaneous_activity=0.02,max_distance=3,random_connection=False)\n",
    "    raster_df = btw.collect_raster_data(1000)\n",
    "    df = raster_to_basic(raster_df)\n",
    "    df_transmission = raster_to_transmission(raster_df)\n",
    "    avalanche = transmission_to_avalanche(df_transmission)\n",
    "    avalanches.append(avalanche)\n",
    "all_avalanches = [item for sublist in avalanches for item in sublist]\n",
    "print(all_avalanches)\n",
    "avalanches_by_length = defaultdict(list)\n",
    "for av in all_avalanches:\n",
    "    avalanches_by_length[len(av)].append(av)\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the all_avalnches to csv\n",
    "avalanche_df = avalanche_to_statistics(all_avalanches)\n",
    "avalanche_df.to_csv(\"data/avalanche_statistics_scalefree.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avalanche size vs. avalanche duration for scale-fee property\n",
    "df = load_data_csv(\"data/avalanche_statistics_scalefree.csv\")\n",
    "avalanches_by_length = defaultdict(list)\n",
    "for av in all_avalanches:\n",
    "    avalanches_by_length[len(av)].append(av)\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}\n",
    "plt.figure(figsize=(6, 5)) \n",
    "plt.title(\"Scale-free Property\", fontsize=14)\n",
    "plt.xlabel(\"Avalanche Durations\", fontsize=14)\n",
    "plt.ylabel(\"Avalanche Sizes\", fontsize=14)\n",
    "plt.grid(True)\n",
    "for length, mean_activity in mean_activities.items():\n",
    "    if length < 17 and length > 3:\n",
    "        plt.plot(range(length), mean_activity, label=f'Duration {length}', color='blue', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerlaw scatter for sizes\n",
    "avalanche_sizes = [sum(av) for av in all_avalanches]\n",
    "size_counts = Counter(avalanche_sizes)\n",
    "sizes, counts = zip(*size_counts.items())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Avalanche Size Distribution\")\n",
    "plt.xlabel(\"Avalanche size (log scale)\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "log_sizes = np.log(sizes)\n",
    "log_counts = np.log(counts)\n",
    "\n",
    "plt.scatter(log_sizes, log_counts, color='blue', marker='o')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerlaw fitting for sizes\n",
    "avalanche_sizes = [sum(av) for av in all_avalanches if len(av) > 0]  \n",
    "\n",
    "fit = powerlaw.Fit(avalanche_sizes)\n",
    "fig = fit.plot_pdf(color='b', linewidth=2, label=\"Original Distribution\")\n",
    "fit.power_law.plot_pdf(color='r', linestyle='--', ax=fig, label=\"Powerlaw Fitting Distribution\")\n",
    "t = fit.power_law.alpha\n",
    "plt.xlabel(\"Avalanche Size\", fontsize=14)\n",
    "plt.ylabel(\"Probability Density\", fontsize=14)\n",
    "plt.title(\"Avalanche Dize Distribution\", fontsize=16)\n",
    "plt.legend(fontsize = 13)\n",
    "\n",
    "print('Tau (τ) - Power-law exponent:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "mean_activities = {length: np.mean(np.array(avs), axis=0) for length, avs in avalanches_by_length.items()}\n",
    "durations = []\n",
    "sizes = []\n",
    "for duration, activities in mean_activities.items():\n",
    "    if duration < 17 and duration > 5:\n",
    "        durations.append(duration)\n",
    "        sizes.append(np.mean(activities))\n",
    "log_durations = np.log(durations).reshape(-1, 1)\n",
    "log_sizes = np.log(sizes)\n",
    "model = LinearRegression()\n",
    "model.fit(log_durations, log_sizes)\n",
    "predicted = model.predict(log_durations)\n",
    "gamma = model.coef_[0]\n",
    "r2 = r2_score(log_sizes, predicted)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(log_durations, log_sizes, color='black', label='Data points')\n",
    "plt.plot(log_durations, predicted, color='red', label=f'Linear fit: γ = {gamma:.2f}, R² = {r2:.2f}')\n",
    "plt.legend()\n",
    "plt.xlabel('Log(duration)')\n",
    "plt.ylabel('Log(size)')\n",
    "plt.title('Log-Log Plot of Avalanche Duration vs Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the scalefree with zooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5)) \n",
    "plt.title(\"Scale-free Property\", fontsize=14)\n",
    "plt.xlabel(\"Avalanche Durations\", fontsize=14)\n",
    "plt.ylabel(\"Avalanche Sizes\", fontsize=14)\n",
    "plt.grid(True)\n",
    "for length, mean_activity in mean_activities.items():\n",
    "    if length < 17 and length > 5 :\n",
    "        scaled_time = [i / (length - 1) for i in range(length)]\n",
    "        scaled_activity = [activity / ((length) ** (0.89 - 1)) for activity in mean_activity]\n",
    "        plt.plot(scaled_time, scaled_activity, label=f'Duration {length}', color='blue', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 3, 3]\n",
    "b = [0, 3, 2]\n",
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "args = {\"one\": 4, \"two\": 5, \"three\": 0.02}\n",
    "args_df = pd.DataFrame(args, index=[0])\n",
    "results_df = pd.DataFrame({\"spikes_total\": np.array([0, 3, 3]), \n",
    "                        \"spikes_neighbours\": np.array([0, 1, 1]), \n",
    "                        \"spikes_input\": np.array([0, 3, 3]) - np.array([0, 1, 1])})\n",
    "combined_df = pd.concat([args_df, results_df], axis=1)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_data_csv(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic for presentation:\n",
    "### Motivation\n",
    "### Data we use: self-generated?\n",
    "### Hypothesis\n",
    "### What we did:\n",
    "#### CA Model: \n",
    "##### How is this Model like？\n",
    "1. Rules: \n",
    "   1. We have a 2d grid. \n",
    "   2. Interact rule: The neighbour neurons around spikes are going to be added 1 to their grid number. If in ths time step, neurons who get enough gird number, which means reaches the threshold, becomes a spike_neighbour in the next time step. And if they donot reach the threshold, their grid numbers will be cleared to zero.\n",
    "   3. Refractory period: After 1 timestep, a spike will get into the refractory period, which means within the next refractoy period timesteps, the spike will get activated whatever happens.\n",
    "   4. Spontaneous spike rule: We have this spontaneous_input_probability p to make random unactivated and also not in refractory period neurons to activate and become a spike_input.\n",
    "   5. So, we have a big timestep for loop, we first make all spikes into refractory period and also make spikes in refractory period in time -1 refractory period. This will take effect at the next timestep. Then we check neighbours to create spikes_neighbours and then we add_grains to make spikes_input.\n",
    "2. Parameters(max_height,refractory_period,spontaneous_input_probability,random_connection，max_distance)\n",
    "##### What we get: \n",
    "1. By playing with the parameters tuning, we find patterns(vedios and raster figure). \n",
    "2. While running on different settings of parameters, we find the phase transition around branching ratio =1.\n",
    "3. While changing only one parameter (let's say a) each time, we find the interesting relationship beween spike_density and a. In principle, what we are doing is changing branching ratio. However, the relationship is different for different a. You see it looks like first-order phase transition, but we think actually it's not. It may be just because of our activation rule is adding 1 to your neighbours at one time, but not 0.3 or 0.5. So the data we get is discrete.\n",
    "4. We tried to delve into the powerlaw distribution of avalanche size and distribution and the relationship between these 2 /tao, but the results we get is for avalanche size, it clearly follows powerlaw distribution, but for avalanche duration distribution, it seems not. We think it's due to in the CA model we implemented, the avalanches tend to merge into one big avalanche. Even though we have implemented a complicated algorithm to track avalanche origins, we still cannot prevent avalanches merging into one big avalanche. So the avalanche data we get often ends with one very big avalanche size and duration number. \n",
    "So we want to implement this branching model with random network to make up for the shortcomings of existing models in studying avalanche size and duration distribution.\n",
    "#### Branching model\n",
    "How is the model like: Rule, Parameters\n",
    "What we get:"
    "### Testing varying parameters to catch the phase transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying maximum heights (thresholds).\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_values = range(1, 10)\n",
    "average_densities = []\n",
    "\n",
    "for height in height_values:\n",
    "    file_name = f\"data/new_varying_height_{height}.csv\"\n",
    "    data = pd.read_csv(file_name)\n",
    "\n",
    "    average_density = data['spikes_total'].mean() / (50 * 50)\n",
    "    average_densities.append(average_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_values = range(1, 10)\n",
    "refractory_periods = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for refractory_period in refractory_periods:\n",
    "    average_densities = []\n",
    "\n",
    "    for height in height_values:\n",
    "        file_name = f\"data/new_varying_height_{height}_ref_{refractory_period}.csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "\n",
    "        average_density = data[\"spikes_total\"].mean() / (50 * 50)\n",
    "        average_densities.append(average_density)\n",
    "\n",
    "    plt.plot(height_values, average_densities, marker='o', label=f'Refractory {refractory_period}')\n",
    "\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Average Density of Active Neurons')\n",
    "plt.title('Density-Height Phase Transitions for Different Refractory Periods')\n",
    "\n",
    "plt.legend(title=\"Refractory Periods\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_params_by_ref = {rp: [] for rp in refractory_periods}\n",
    "\n",
    "for rp in refractory_periods:\n",
    "    for height in height_values:\n",
    "        file_name = f\"data/new_varying_height_{height}_ref_{rp}.csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "        sigma= branching_prameter(data)\n",
    "        branching_params_by_ref[rp].append(sigma)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for rp, branching_params in branching_params_by_ref.items():\n",
    "    plt.plot(height_values, branching_params, marker='o', label=f'Refractory{rp}')\n",
    "\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Branching Parameter')\n",
    "plt.title('Branching Parameter vs. Height for Different Refractory Periods')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(height_values, average_densities, marker='o')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Average Density of Active Neurons')\n",
    "plt.title('Density-Height Phase Transitions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_params = []\n",
    "\n",
    "for height in height_values:\n",
    "    filename = f\"data/new_varying_height_{height}.csv\"\n",
    "    data = pd.read_csv(filename)\n",
    "    sigma = branching_prameter(data)\n",
    "    branching_params.append(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(height_values, branching_params, marker='o')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Branching Parameter')\n",
    "plt.title('Branching Parameter vs. Height')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_to_check = [3, 4]\n",
    "branching_parameters = {}\n",
    "\n",
    "for height in height_to_check:\n",
    "    file_name = f\"data/varying_height_{height}.csv\"\n",
    "    data = pd.read_csv(file_name)\n",
    "    sigma = branching_prameter(data)  # Ensure the function name is correctly spelled as 'branching_parameter'\n",
    "    branching_parameters[height] = sigma\n",
    "\n",
    "print(branching_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refractory_period_values = range(1, 10)\n",
    "average_densities_1 = []\n",
    "\n",
    "for refractory_period in refractory_period_values:\n",
    "    filename = f\"data/new_varying_refractory_{refractory_period}.csv\"\n",
    "    data_1 = pd.read_csv(filename)\n",
    "\n",
    "    average_density_1 = data_1['spikes_total'].mean() / (50 * 50)\n",
    "    average_densities_1.append(average_density_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(refractory_period_values, average_densities_1, marker='o')\n",
    "plt.title(\"Refractory Period vs. Average Density of Active Neurons\")\n",
    "plt.xlabel(\"Refractory Period\")\n",
    "plt.ylabel(\"Average Density of Active Neurons\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refractory_period_to_check = [3, 4]\n",
    "branching_parameters = {}\n",
    "\n",
    "for refractory_period in refractory_period_to_check:\n",
    "    file_name = f\"data/new_varying_refractory_{refractory_period}.csv\"\n",
    "    data = pd.read_csv(file_name)\n",
    "    sigma = branching_prameter(data)\n",
    "    branching_parameters[refractory_period] = sigma\n",
    "\n",
    "print(branching_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_params = []\n",
    "\n",
    "for refractory_period in refractory_period_values:\n",
    "    filename = f\"data/new_varying_refractory_{refractory_period}.csv\"\n",
    "    data = pd.read_csv(filename)\n",
    "    sigma = branching_prameter(data)\n",
    "    branching_params.append(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(refractory_period_values, branching_params, marker='o')\n",
    "plt.xlabel('Refractory Period')\n",
    "plt.ylabel('Branching Parameter')\n",
    "plt.title('Branching Parameter vs. Refractory Period')\n",
    "plt.grid(True)\n",
    "plt.ylim([min(branching_params) - 0.6, max(branching_params) + 0.6])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputationalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
